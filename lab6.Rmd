---
title: "Lab6"
author: "Javier Patron"
date: "`r Sys.Date()`"
output: html_document
---


## Case Study Eel Species Distribution Modeling
This week's lab follows a modeling project described by Elith et al. (2008) (Supplementary Reading)

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(caret)
library(vip)
library(xgboost)
library(reshape2)
library(tictoc)

```


## Data
Grab the model training data set from the class Git:
data/eel.model.data.csv

```{r}
eel_data <- read_csv(here::here("eel.model.data.csv")) |> 
  clean_names() |> 
  mutate(angaus = as.factor(angaus))

```


### Split and Resample
Split the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus

```{r}
#Split the data
eel_split <- initial_split(eel_data, strata = "angaus")

# Crate the testing and testing data
eel_train <- training(eel_split)
eel_test <- testing(eel_split)


# Create the10 fold CV
eel_cv = eel_train |> vfold_cv(v = 10)

```

### Preprocess

Create a recipe to prepare your data for the XGBoost model.  We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis

```{r}
eel_recipe <- recipe(angaus ~ .,
                     data = eel_train) |> 
  step_integer(all_predictors(), zero_based = TRUE) |> 
  prep() |> 
  bake(new_data = eel_train)

```


### Tune Learning Rate

Following the XGBoost tuning strategy outlined on Monday, first we conduct tuning on just the `learn_rate` parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()
```{r}
xg_model_learn <-parsnip::boost_tree(
  mode = "classification",
  trees = 20, # Recommended 3,000
  learn_rate = tune(), #eta
) |> 
  set_engine("xgboost") # Set the model that you want to use.

```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: recommended: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
```{r}
set.seed(123)
xg_grid_learn <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

```


-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run. You could use {tictoc} or Sys.time().

Create a workflow 
```{r}

xgWorkflow_learn <- 
  workflows::workflow() %>%
  add_model(xg_model_learn) %>% 
  add_formula(angaus ~ .)

```

Tune your grid with tune_grid. (tune_grid() runs a set of performance metrics (e.g. accuracy or RMSE) for a pre-defined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data.

```{r}
tic()
xg_learn_tuned <- tune_grid(
  object = xgWorkflow_learn,
  resamples = eel_cv,
  grid      = xg_grid_learn,
  metrics   = metric_set(roc_auc),
  control   = control_grid(verbose = TRUE))

toc()

```

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}

xg_learn_tuned %>% tune::show_best(metric = "roc_auc")
best_learn <- xg_learn_tuned %>% tune::select_best(metric = "roc_auc")

```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters (trees = 3,000. tree_depth, min_n, loss_reduction)

```{r}
xgmodel_three <-parsnip::boost_tree(
  mode = "classification",
  trees = 30, # Recommended 3,000
  learn_rate = best_learn$learn_rate, #eta
  min_n = tune(), #min_child_weight
  tree_depth = tune(), #max_depth
  loss_reduction = tune() #loss 
  ) |> 
  set_engine("xgboost")

```

Set up the parameters
```{r}
xgboostParams <- dials::parameters(
  min_n(),
  tree_depth(),
  loss_reduction())
```

Create a workflow 
```{r}
xg_workflow_three <- 
  workflows::workflow() %>%
  add_model(xgmodel_three) %>% 
  add_formula(angaus ~ .)

```


2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space

```{r}
xg_grid_maxent <- dials::grid_max_entropy(xgboostParams, size = 20) #GRID specs from the book
```

Use the tune_grid to feed all alternatives
```{r}
tic()

xg_three_tuned <- tune_grid(
  object = xg_workflow_three,
  resamples = eel_cv,
  grid      = xg_grid_maxent ,
  metrics   = metric_set(roc_auc),
  control   = control_grid(verbose = TRUE))

toc()
```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
xg_three_tuned %>% tune::show_best(metric = "roc_auc")

three_metrics <- xg_three_tuned %>% tune::select_best(metric = "roc_auc")

```

### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized. lean_rate, min_n, tree_depth, loss_reduction) and tune the stochastic parameters.

```{r}
stochastic_model <-parsnip::boost_tree(
  mode = "classification",
  trees = 30, # Recommended 3,000
  learn_rate = best_learn$learn_rate,
  tree_depth = three_metrics$tree_depth, #max_depth
  min_n = three_metrics$min_n, #min_child_weight
  loss_reduction = three_metrics$loss_reduction, #loss
  sample_size = tune(), #stochastic
  mtry = tune() #colsample_by tree stochastic
) |> 
  set_engine("xgboost")
```

Set up the new parameters
```{r}
stochastic_Params <- dials::parameters(
  sample_size = sample_prop(),
  finalize(mtry(), eel_train))

```

Create a workflow 
```{r}
stochastic_workflow <- workflows::workflow() %>%
  add_model(stochastic_model) %>% 
  add_formula(angaus ~ .)

```


2.  Set up a tuning grid. Use grid_max_entropy() again.
```{r}

stochastic_grid <- dials::grid_max_entropy(stochastic_Params, size = 20) #GRID specs from the book

```

```{r}

stochastic_tuned <- tune_grid(
  object = stochastic_workflow,
  resamples = eel_cv,
  grid      = stochastic_grid ,
  metrics   = metric_set(roc_auc),
  control   = control_grid(verbose = TRUE))

```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
stochastic_tuned %>% tune::show_best(metric = "roc_auc")
stochastic_metrics <- stochastic_tuned %>% tune::select_best(metric = "roc_auc")

```

## Finalize workflow and make final prediction

```{r}
#try autoplot()

final_model <-parsnip::boost_tree(
  mode = "classification",
  trees = tune(), # Recommended 3,000
  learn_rate = best_learn$learn_rate,
  tree_depth = three_metrics$tree_depth, #max_depth
  min_n = three_metrics$min_n, #min_child_weight
  loss_reduction = three_metrics$loss_reduction, #loss
  sample_size = stochastic_metrics$sample_size, #stochastic
  mtry = stochastic_metrics$mtry,
  stop_iter = tune()
  #colsample_by tree stochastic
) |> 
  set_engine("xgboost")

# Create your workflow
final_workflow <- workflows::workflow() %>%
  add_model(final_model) %>% 
  add_formula(angaus ~ .)

# Create your final parameters
final_Params <- dials::parameters(stop_iter(),
                                  trees())

# Create your grid
final_grid <- dials::grid_max_entropy(final_Params, size = 20) 

# Run your final model

final_tuned <- tune_grid(
  object = final_workflow,
  resamples = eel_cv,
  grid      = final_grid ,
  metrics   = metric_set(roc_auc),
  control   = control_grid(verbose = TRUE))


```


1.  Assemble your final workflow will all of your optimized parameters and do a final fit.

```{r}

xg_model_learn %>% collect_metrics() %>% 
  select(mean, mtry:sample_size) %>% 
  melt(id="mean") %>% 
  ggplot(aes(y = mean,
             x = value,
             colour = variable)) + 
  geom_point(show.legend = FALSE) + 
  facet_wrap(variable~. , scales="free") + theme_bw() +
  labs(y="Mean log-loss", x = "Parameter")

```

2. How well did your model perform? What types of errors did it make?

**Written model: **

## Fit your model the evaluation data and compare performance

1.  Now fit your final model to the big dataset: data/eval.data.csv

```{r}
# Finalize the model
final_boosted <- finalize_model(final_model, 
                             select_best(final_tuned))

# Fit the model to the testing data!
final_forest_fit <- last_fit(final_boosted, 
                           angaus ~ . , 
                           eel_split) # eel_split or eel_test
```


lab5, just predictions. dont do the full dataset.

collection_metric (accuracy) 

2.  How does your model perform on this data?

3.  How do your results compare to those of Elith et al.?


-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?


```{r}
orig_var_imp <- final_wf |>
  fit(data = eel_model_data) |>
  pull_workflow_fit() |>
  vip(geom = "col", num_features = 12) + 
  labs(title = "Original Data")
```



